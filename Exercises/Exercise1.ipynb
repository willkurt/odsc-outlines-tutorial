{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541e1e6c-c170-4572-8844-01f35ba443a3",
   "metadata": {},
   "source": [
    "# Exercise 1 - Building a 0-shot classifier with Outlines\n",
    "\n",
    "In this notebook we will take our first look at how to use [Outlines](https://github.com/outlines-dev/outlines). Most of the code is written for you, but you will be required to implement small snippets of code to make sure you understand the key concepts being discussed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39697032-a6fe-4342-a50a-5185df3ee3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import outlines\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda33d6-3f07-47ca-99e3-3416dbf63f3a",
   "metadata": {},
   "source": [
    "Here we load our model using `outlines.models.transformers`.\n",
    "\n",
    "**Note**: If you are not using an Apple Silicon device, you should change `mps` to either `cuda` if you have an NVIDIA GPU or `cpu` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32b83f-df5f-4d81-a5df-d568e0c91566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "model = outlines.models.transformers(\n",
    "    model_name,\n",
    "    device='mps',\n",
    "    model_kwargs={\n",
    "        'torch_dtype': torch.bfloat16,\n",
    "        'trust_remote_code': True\n",
    "    })\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b04424-18cd-403e-b9b9-299d5136f768",
   "metadata": {},
   "source": [
    "## The Task: Predicting Department\n",
    "\n",
    "In this example we'll be processing messages that have received involving user complaints, here is the data loaded with a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcc770-ddff-47c1-92f9-41fb646163cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../examples.json\",'r') as fin:\n",
    "    complaint_data = json.loads(fin.read())\n",
    "complaint_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a2000-7d12-42bd-91ac-6b9d57e6647a",
   "metadata": {},
   "source": [
    "As you can see, there are departments associated with each complaint. What we want to do is build a **zero-shot** classifier to predcit the department given the content of the `message`. Here are the departments in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2037fb-d172-4983-bd0d-d6d94de3c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = [\"clothing\",\"electronics\",\"kitchen\",\"automotive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be1e1d-334d-4b12-bfb1-a016be7a2c44",
   "metadata": {},
   "source": [
    "Zero-shot classifiers are great use cases for LLMs because they allow us to try to classify data with *zero* example cases. This is particularly helpful when we don't have any existing data to train a traditional machine learning classifier on. For the purpose of this workshop we're using defaulting to the small but not very powerful `Qwen/Qwen2-0.5B-Instrct` model. This is to ensure this model runs reasonably well on most laptops. This model performs relatively poorly on the task, but if we were to upgrade to something like `Phi-3-medium` we can get 92% accuracy on this task *with no traning examples required!*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de3893-b156-4b02-9ef5-5d21c2e68514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:21:47.586428Z",
     "iopub.status.busy": "2024-08-16T18:21:47.586361Z",
     "iopub.status.idle": "2024-08-16T18:21:47.588067Z",
     "shell.execute_reply": "2024-08-16T18:21:47.587872Z",
     "shell.execute_reply.started": "2024-08-16T18:21:47.586420Z"
    }
   },
   "source": [
    "### Our Prompt\n",
    "\n",
    "We are going to be using an `instruct` model which means we can use a *instruction* or *chat-based* style of prompting. The function below takes our complaint and formats it into a prompt to send to the LLM. Note the use of `tokenizer.apply_chat_template`. Each instruct LLM has a slightly different format for it's instruction prompts, but using `apply_chat_template` makes it easy to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328bc75b-5e9e-488c-9715-9e40f8f61146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(complaint):\n",
    "    prompt_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are as agent designed to help label complaints.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        I'm going to provide you with a consumer complaint to analyze.\n",
    "        The complaint is going to be regarding a product from one of our\n",
    "        departments. Here is the list of departments:\n",
    "            - \"clothing\"\n",
    "            - \"electronics\"\n",
    "            - \"kitchen\"\n",
    "            - \"automotive\"\n",
    "        Please reply with *only* the name of the department.\n",
    "        \"\"\")\n",
    "    },{\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I understand and will only answer with the department name\"\n",
    "    },{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Great! Here is the complaint: {complaint['message']}\"\n",
    "    }\n",
    "                       \n",
    "                      ]\n",
    "    prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4e409-2302-4d2c-b654-8f2033ea0f44",
   "metadata": {},
   "source": [
    "This is how the first complaint looks when rendered as a prompt for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2cda2-fb64-4b77-9361-b00528322874",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prompt(complaint_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f375d-f8a9-4b82-883b-ba42714136cf",
   "metadata": {},
   "source": [
    "## Unstructured Generation\n",
    "\n",
    "To demonstrate the value of *structured* generation we'll first start with an *unstructured* example. We can still use Outlines for this! In this case we'll use `outlines.generate.text` to create a text generator using our LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a0b08-5198-4c3e-9ab7-ffc3b816eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = outlines.generate.text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775eda2-303d-4684-be5a-fff5edcf9875",
   "metadata": {},
   "source": [
    "Next we iterate through a few examples to see what our unstructured response looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e445702-41e8-41b6-8341-dd86231d3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for complaint in complaint_data[0:4]:\n",
    "    prompt = create_prompt(complaint)\n",
    "    result = generator(prompt, max_tokens=12)\n",
    "    print(f\"LLM: {result} Actual: {complaint['department']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb62f94-d7cc-4f2d-b185-b9fc8bfb7663",
   "metadata": {},
   "source": [
    "As you can see the results are a bit of mess! In this case we'll need to do a bit of additional parse to out the actual answer (if there is one).\n",
    "\n",
    "Using Structured Generation with Outlines we no longer have to worry about this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afce2f-bd36-4750-802b-a6200c51e47b",
   "metadata": {},
   "source": [
    "## Structured Generation\n",
    "\n",
    "This first exercise is quite simple, you are going to use `outlines.generate.choice` to create a structured generator that will product only structured text. Replace the:\n",
    "\n",
    "```\n",
    "generator_struct = None\n",
    "```\n",
    "\n",
    "With the correct code to solve this problem with `choice`. This task should be very easy, but it's a good first start at using Outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1682f-ce41-4136-ad17-f0dea4c4e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "# Exercise 1 is just to fill in this code.\n",
    "generator_struct = None\n",
    "for complaint in complaint_data:\n",
    "    prompt = create_prompt(complaint)\n",
    "    result = generator_struct(prompt)\n",
    "    if result == complaint['department']:\n",
    "        correct += 1\n",
    "    print(f\"LLM: {result} Actual: {complaint['department']}\")\n",
    "print(correct/len(complaint_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f3f8b-7b21-4094-8454-7b0e32ecc5cf",
   "metadata": {},
   "source": [
    "Depending on the underlying LLM you use, you may get dramatically different performance but in all cases you should find that the model *always* produces just a single, valid department name as an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0733e-f29d-4503-a315-c0dff0c6a3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
