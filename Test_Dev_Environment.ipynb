{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d65d8f-e189-4a2d-b190-7568933ffaba",
   "metadata": {},
   "source": [
    "# Testing Your Dev Environment\n",
    "\n",
    "The purpose of this notebook is to ensure that your development environment for this workshop is setup correctly.\n",
    "\n",
    "## Basic Libraries\n",
    "\n",
    "Let's start by making sure all of the libraries are installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39697032-a6fe-4342-a50a-5185df3ee3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:39.429332Z",
     "iopub.status.busy": "2024-08-16T18:19:39.429160Z",
     "iopub.status.idle": "2024-08-16T18:19:40.657875Z",
     "shell.execute_reply": "2024-08-16T18:19:40.657611Z",
     "shell.execute_reply.started": "2024-08-16T18:19:39.429317Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import outlines\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b102c66-24f2-4b51-b232-288e9def0cdb",
   "metadata": {},
   "source": [
    "## Loading models\n",
    "\n",
    "This tutorial was built using `microsoft/Phi-3-medium-4k-instruct` but this can be too large for most laptops/notebook environments. To make this easier there are two good altneratives to Phi-3-medium:\n",
    "\n",
    "- `microsoft/Phi-3-mini-4k-instruct`: Smaller (but still several GB) model that performs reasonably well.\n",
    "- `Qwen/Qwen2-0.5B-Instruct`: Very small model, should run on most machine (the default for the notebook).\n",
    "\n",
    "Unless you have a particularly powerful and high RAM macbook pro that can handle the large models, it is highly recommended that you stick with Qwen2 for the exercises. The results will be fairly mediocre, but it should be very easy to transfer all the skills learned in the exercises to more powerful models either on a desktop at home or hosted in the cloud.\n",
    "\n",
    "Here is the basic code to load the model and tokenizer (using for generating instruct prompts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31252076-d9da-48dd-8cf2-1a04c84a8ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:40.658821Z",
     "iopub.status.busy": "2024-08-16T18:19:40.658659Z",
     "iopub.status.idle": "2024-08-16T18:19:42.214397Z",
     "shell.execute_reply": "2024-08-16T18:19:42.214124Z",
     "shell.execute_reply.started": "2024-08-16T18:19:40.658813Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "model = outlines.models.transformers(\n",
    "    model_name,\n",
    "    # this assumes Apple Silicon\n",
    "    # Remove or change to 'cuda'/'cpu' if you\n",
    "    # are using another device.\n",
    "    device='mps',\n",
    "    model_kwargs={\n",
    "        'torch_dtype': torch.bfloat16,\n",
    "        'trust_remote_code': True\n",
    "    })\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67cd2f8-812c-4751-a597-5061c52468aa",
   "metadata": {},
   "source": [
    "Then we create a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e755c8-36eb-4d06-a991-f2ecec8904ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:42.214955Z",
     "iopub.status.busy": "2024-08-16T18:19:42.214804Z",
     "iopub.status.idle": "2024-08-16T18:19:42.220937Z",
     "shell.execute_reply": "2024-08-16T18:19:42.220756Z",
     "shell.execute_reply.started": "2024-08-16T18:19:42.214944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGenerate a phone number<|im_end|>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a phone number\"\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f375d-f8a9-4b82-883b-ba42714136cf",
   "metadata": {},
   "source": [
    "## Unstructured Generation\n",
    "\n",
    "This code will test *unstructured* generation with outlines. If you can run this in a reasonable amount of time you have selected the appropiate sized model for you laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957a0b08-5198-4c3e-9ab7-ffc3b816eee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:42.221343Z",
     "iopub.status.busy": "2024-08-16T18:19:42.221275Z",
     "iopub.status.idle": "2024-08-16T18:19:42.222926Z",
     "shell.execute_reply": "2024-08-16T18:19:42.222667Z",
     "shell.execute_reply.started": "2024-08-16T18:19:42.221336Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = outlines.generate.text(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ca5118-6532-41e9-b989-c62c2808126f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:42.224120Z",
     "iopub.status.busy": "2024-08-16T18:19:42.224051Z",
     "iopub.status.idle": "2024-08-16T18:19:42.919961Z",
     "shell.execute_reply": "2024-08-16T18:19:42.919711Z",
     "shell.execute_reply.started": "2024-08-16T18:19:42.224114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'茫然'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt,max_tokens=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6f6e0-4234-4cb7-8f55-a3c34ed9c7ee",
   "metadata": {},
   "source": [
    "## Structured Generation\n",
    "\n",
    "Finally we run an example using *structured* generation. If you can run this code, then everything should run fine for the rest of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6db5ce-035c-46ae-a849-4ed4b6d2016b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:42.920376Z",
     "iopub.status.busy": "2024-08-16T18:19:42.920300Z",
     "iopub.status.idle": "2024-08-16T18:19:46.002120Z",
     "shell.execute_reply": "2024-08-16T18:19:46.001691Z",
     "shell.execute_reply.started": "2024-08-16T18:19:42.920367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Compiling FSM index for all state transitions: 100%|████| 15/15 [00:00<00:00, 33.31it/s]\n"
     ]
    }
   ],
   "source": [
    "generator_struct = outlines.generate.regex(model, r\"\\([0-9]{3}\\) [0-9]{3}-[0-9]{4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e057800c-066e-43a2-9620-51e3a40a9029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T18:19:46.002921Z",
     "iopub.status.busy": "2024-08-16T18:19:46.002763Z",
     "iopub.status.idle": "2024-08-16T18:19:47.084926Z",
     "shell.execute_reply": "2024-08-16T18:19:47.084674Z",
     "shell.execute_reply.started": "2024-08-16T18:19:46.002911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(100) 555-1234'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_struct(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1798a7-cb06-4cbf-8d4c-2e0979208ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
